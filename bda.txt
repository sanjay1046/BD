#!/usr/bin/env python
# -*- coding: utf-8 -*-
# mapper.py (Python 2.6 Compatible)

import sys
import time
import resource # For memory and CPU time

# --- Flajolet-Martin (FM) Algorithm ---

def get_trailing_zeros(n):
    """Gets the number of trailing zeros in the binary representation of n."""
    if n == 0:
        return 32 # Assume 32-bit hash
    
    # Use bin() for binary string and strip 0s
    try:
        s = bin(n)
        return len(s) - len(s.rstrip('0'))
    except:
        return 0

class FMEstimator:
    """Implements the basic Flajolet-Martin algorithm."""
    def __init__(self):
        # Our bitmap is just a single integer
        self.bitmap = 0
    
    def add(self, item):
        # Use Python 2's built-in hash().
        # We need a non-negative hash.
        h = abs(hash(item))
        
        # Find the position of the least-significant 1-bit
        # (which is the same as the number of trailing 0s)
        z = get_trailing_zeros(h)
        
        # Set the z-th bit in our bitmap to 1
        self.bitmap = self.bitmap | (1 << z)
        
    def estimate(self):
        if self.bitmap == 0:
            return 0
        
        # Find R, the position of the first 0-bit
        r = 0
        temp_bitmap = self.bitmap
        while (temp_bitmap & 1) == 1:
            temp_bitmap = temp_bitmap >> 1
            r += 1
        
        # The FM estimator is 2^R / phi
        phi = 0.77351
        return int(pow(2, r) / phi)

# --- Main Mapper Logic ---

def run_mapper():
    fm = FMEstimator()
    exact_set = set()
    total_lines = 0
    
    # --- Start Tracking Performance ---
    start_time = time.time()
    start_cpu = resource.getrusage(resource.RUSAGE_SELF).ru_utime
    
    # Read from standard input
    for line in sys.stdin:
        total_lines += 1
        try:
            parts = line.strip().split('\t')
            if len(parts) < 4:
                continue # Skip malformed lines
            
            # Create a unique key for the variant
            # (chrom, pos, ref, alt)
            key = "{0}_{1}_{2}_{3}".format(parts[0], parts[1], parts[2], parts[3])
            
            # 1. Add to exact set
            exact_set.add(key)
            
            # 2. Add to FM estimator
            fm.add(key)
            
        except Exception as e:
            # Ignore any errors in the stream
            pass 
    
    # --- Stop Tracking Performance ---
    end_time = time.time()
    end_cpu = resource.getrusage(resource.RUSAGE_SELF).ru_utime
    
    # Get memory in MB (this is platform-dependent, ru_maxrss is good for Linux)
    mem_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1024.0) # KB
    mem_usage_mb = mem_usage / 1024.0 # MB
    
    # Get counts
    actual_count = len(exact_set)
    predicted_count = fm.estimate()
    
    # --- Pass all data to the reducer ---
    # We use key-value pairs separated by a tab
    # FIXED FOR PYTHON 2.6
    print "{0}\t{1}".format("actual_count", actual_count)
    print "{0}\t{1}".format("predicted_count", predicted_count)
    print "{0}\t{1}".format("total_lines", total_lines)
    print "{0}\t{1}".format("processing_time_s", end_time - start_time)
    print "{0}\t{1}".format("cpu_time_s", end_cpu - start_cpu)
    print "{0}\t{1}".format("memory_usage_mb", mem_usage_mb)

if __name__ == "__main__":
    run_mapper()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
# reducer.py (Python 2.6 Compatible)

import sys
import math

def run_reducer():
    data = {}
    # Read the data from the mapper
    for line in sys.stdin:
        try:
            key, value = line.strip().split('\t', 1)
            data[key] = float(value)
        except ValueError:
            pass # Ignore malformed lines
    
    # --- Extract Values ---
    A = data.get("actual_count", 0.0)
    P = data.get("predicted_count", 0.0)
    total_lines = data.get("total_lines", 0.0)
    proc_time = data.get("processing_time_s", 1.0) # Avoid div by zero
    cpu_time = data.get("cpu_time_s", 0.0)
    mem_mb = data.get("memory_usage_mb", 0.0)
    
    # Handle edge case of empty input
    if A == 0:
        print "No data received."
        sys.exit()
    if proc_time == 0:
        proc_time = 1.0 # Avoid division by zero
            
    # --- Calculate Metrics (based on your image) ---
    
    # Recall = Predicted / Actual
    recall = P / A
    # Precision = Predicted / Predicted (which is 1.0)
    precision = 1.0000 
    
    # Approximation Error = |A - P| / A
    approx_error = abs(A - P) / A
    # Loss Function (MSE) = (A - P)^2
    loss_mse = pow(A - P, 2)
    # Error Rate = Approx Error as percentage
    error_rate_pct = approx_error * 100
    
    # Performance Metrics
    cpu_util = cpu_time / proc_time
    latency_ms = proc_time * 1000
    # Throughput = Total lines processed / time
    throughput = total_lines / proc_time
    
    # Scalability Score = (1 - approx_error) * 10
    scalability = (1.0 - approx_error) * 10.0
    # Fault Tolerance = Hardcoded from your image
    fault_tolerance = 2 
    
    # --- Print Final Summary (FIXED FOR PYTHON 2.6) ---
    print "===================================="
    print "  ðŸ“Š CG COUNT SUMMARY"
    print "===================================="
    print "Actual Total CGs:\t{0:.0f}".format(A)
    print "Predicted Total CGs:\t{0:.0f}".format(P)
    print "------------------------------------"
    print "Precision:\t\t{0:.4f}".format(precision)
    print "Recall:\t\t\t{0:.4f}".format(recall)
    print "Approximation Error:\t{0:.4f}".format(approx_error)
    print "Loss Function (MSE):\t{0:.2f}".format(loss_mse)
    print "Error Rate (%%):\t\t{0:.2f}".format(error_rate_pct)
    print "------------------------------------"
    print "CPU Time (s):\t\t{0:.2f}".format(cpu_time)
    print "CPU Utilization:\t{0:.4f}".format(cpu_util)
    print "Processing Time (s):\t{0:.2f}".format(proc_time)
    print "Job Completion Time (s):{0:.2f}".format(proc_time)
    print "Latency (ms):\t\t{0:.2f}".format(latency_ms)
    print "Throughput (records/sec):{0:.2f}".format(throughput)
    print "Memory Usage (MB):\t{0:.4f}".format(mem_mb)
    print "------------------------------------"
    print "Scalability Score (0-10): {0}".format(scalability)
    print "Fault Tolerance:\t{0}".format(fault_tolerance)
    print "===================================="

if __name__ == "__main__":
    run_reducer()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
# dgim.py (Python 2.6 Compatible)

import sys
from collections import deque

class DGIM:
    """Implements the Datar-Gionis-Indyk-Motwani (DGIM) algorithm."""
    
    def __init__(self, N, r=2):
        self.N = N  # Window size
        self.r = r  # Max buckets of same size
        self.timestamp = 0
        # Buckets are stored as (timestamp, size)
        self.buckets = [] 

    def add(self, bit):
        self.timestamp += 1
        
        # --- Window Maintenance ---
        # Remove buckets that have fallen out of the window
        # We only need to check the oldest bucket
        while self.buckets and self.buckets[0][0] <= (self.timestamp - self.N):
            self.buckets.pop(0)

        if bit == 0:
            return # Nothing to add
        
        # If bit is 1, add a new bucket of size 1
        self.buckets.append((self.timestamp, 1))
        self._merge_buckets()

    def _merge_buckets(self):
        """Checks for and merges buckets of the same size."""
        i = len(self.buckets) - 1
        while i >= 0:
            # Count buckets of the same size
            size = self.buckets[i][0]
            count = 0
            j = i
            while j >= 0 and self.buckets[j][1] == self.buckets[i][1]:
                count += 1
                j -= 1
            
            # If we have r+1 buckets, merge two
            if count > self.r:
                # We merge the (r)th and (r+1)th oldest buckets
                # which are at indices j+1 and j+2
                
                # Get timestamp of the second-oldest
                merge_ts = self.buckets[j+2][0]
                merge_size = self.buckets[j+1][1] * 2
                
                # Remove the two old buckets
                self.buckets.pop(j+2)
                self.buckets.pop(j+1)
                
                # Add the new merged bucket
                self.buckets.insert(j+1, (merge_ts, merge_size))
                
                # Start check again from the new bucket
                i = j + 1
            else:
                # This size is fine, move to the next smaller size
                i = j
    
    def query(self, k):
        """Estimates the number of 1s in the last k items."""
        if k > self.N:
            k = self.N
            
        target_ts = self.timestamp - k
        
        total_sum = 0
        last_bucket_size = 0
        
        # Find all buckets that fall at least partially in the window
        for ts, size in reversed(self.buckets):
            if ts > target_ts:
                total_sum += size
                last_bucket_size = size
            else:
                # This is the first bucket to fall out of the window
                # We add half its size as the estimate
                total_sum += (size / 2)
                last_bucket_size = 0 # No more correction needed
                break
        
        # The estimate can be off by at most last_bucket_size / 2
        # A common way to report is the sum minus half the last bucket
        # to guarantee a lower bound (with 50% max error)
        
        if last_bucket_size > 0:
             total_sum -= (last_bucket_size / 2)

        return int(total_sum)

# --- Example Usage ---
if __name__ == "__main__":
    # N = 1000 (window size)
    dgim = DGIM(N=1000)
    
    # We can read from stdin just like the mapper
    line_count = 0
    for line in sys.stdin:
        line_count += 1
        # Add a '1' for every rare variant found
        dgim.add(1)
        
        # Print a report every 100 lines
        if line_count % 100 == 0:
            count_last_500 = dgim.query(500)
            # FIXED FOR PYTHON 2.6
            print "Line {0}: Approx. {1} variants in last 500. (Window size is 1000)".format(
                line_count, count_last_500
            )

    print "--- Final Report ---"
    # FIXED FOR PYTHON 2.6
    print "Total lines processed: {0}".format(line_count)
    print "Approx. variants in last 1000 (full window): {0}".format(dgim.query(1000))
    print "Approx. variants in last 100: {0}".format(dgim.query(100))
